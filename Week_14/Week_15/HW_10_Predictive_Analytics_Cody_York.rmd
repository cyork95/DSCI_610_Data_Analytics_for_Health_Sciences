---
title: "HW_10_Predictive_Health_Analytics_Cody_York"
output: html_notebook
---

# Import the libraries and set the working directory:
```{r}
library(tidyverse)
library(ggplot2)
library(MASS)
library(tree)
library(randomForest)

setwd("C:\\Users\\cy_su\\DataspellProjects\\DSCI_610_Data_Analytics_for_Health_Sciences\\Week_15")
```

# Load the dataset and do initial cleaning.
    I will select only the columns I plan to use for this predicitve analysis. My Quantitative response is going to be 'INCGRP_A' which is the income group of the indiviual. My Binary response is going to be 'DIBEV_A' which says whether the individual is diabetic or not. My Multinomial response is going to be 'EDUC_A' which says how much education the individual has. I will also be pulling the fields 'AGEP_A', 'SEX_A', 'MARITAL_A ', 'REGION', 'HISPALLP_A', 'VISIONDF_A', 'HICOV_A', BMICAT_A, 'HEARINGDF_A', 'DIFF_A', 'COMDIFF_A', 'COGMEMDFF_A' as my predictors.
    I will also be removing all NA values from the dataset and giving them an average value.
```{r}
adult_survey <- read.csv("adult19.csv", header = TRUE)
#head(adult_survey)


adult_survey_cleaned <- subset(adult_survey, select = c(INCGRP_A, DIBEV_A, EDUC_A, AGEP_A, SEX_A, MARITAL_A, REGION, HISPALLP_A,
                                                        VISIONDF_A, BMICAT_A, HEARINGDF_A, DIFF_A, COMDIFF_A, COGMEMDFF_A))


adult_survey_cleaned$DIBEV_A_F[adult_survey_cleaned$DIBEV_A == 1] <- 0
adult_survey_cleaned$DIBEV_A_F[adult_survey_cleaned$DIBEV_A == 2] <- 1
adult_survey_cleaned$DIBEV_A_F <- factor(adult_survey_cleaned$DIBEV_A_F)

head(adult_survey_cleaned)

```

# Create a train / test split for the 3 predictions
```{r}
set.seed(123)

train <- sample(31997, 21997)

predict_income_group <- subset(adult_survey_cleaned, select = c(INCGRP_A, AGEP_A, SEX_A, MARITAL_A, REGION, HISPALLP_A))

predict_diabetes <- subset(adult_survey_cleaned, select = c(DIBEV_A_F, AGEP_A, SEX_A, MARITAL_A, REGION, HISPALLP_A,
                                                            VISIONDF_A, BMICAT_A, HEARINGDF_A, DIFF_A, COMDIFF_A, COGMEMDFF_A))

predict_education <- subset(adult_survey_cleaned, select = c(EDUC_A, AGEP_A, SEX_A, MARITAL_A, REGION, HISPALLP_A,
                                                             VISIONDF_A, BMICAT_A, HEARINGDF_A, DIFF_A, COMDIFF_A, COGMEMDFF_A))

predict_income_group.train <- predict_income_group[train,]
predict_income_group.test <- predict_income_group[-train,]

predict_diabetes.train <- predict_diabetes[train,]
predict_diabetes.test <- predict_diabetes[-train,]
predict_diabetes.test[is.na(predict_diabetes.test)] <- 0

predict_education.train <- predict_education[train,]
predict_education.test <- predict_education[-train,]
```

# Fit a regression model against the 3 predictions and then determine the error for each
```{r}
lm.income.tr <- lm(INCGRP_A ~ AGEP_A + SEX_A + MARITAL_A + REGION + HISPALLP_A, data = predict_income_group.train)
income_test_error <- mean((predict_income_group.test$INCGRP_A - predict(lm.income.tr, predict_income_group.test))^2)
paste("The value of the income test error is:", income_test_error)
```

# Create a regression tree for the quantitative value
```{r}
tree.income <- tree(INCGRP_A ~ AGEP_A + SEX_A + MARITAL_A + REGION + HISPALLP_A, data = predict_income_group.train)
plot(tree.income)
text(tree.income, pretty = 0)

yhat <- predict(tree.income, newdata = predict_income_group.test)
income_tree_test_error <- mean((yhat - predict_income_group.test$INCGRP_A)^2)
paste("The error from the tree regression of the income group is: ", income_tree_test_error)
paste("This new model is more accurate by:", income_test_error - income_tree_test_error)
```
# Create a random forest model for the quantitative value
```{r}
rf.income <- randomForest(INCGRP_A ~ AGEP_A + SEX_A + MARITAL_A + REGION + HISPALLP_A, data = predict_income_group.train)
rf.income

yhat.rf.income <- predict(rf.income, newdata = predict_income_group.test)
income_forest_test_error <- mean((yhat.rf.income - predict_income_group.test$INCGRP_A)^2)
paste("The error from the forest regression of the income group is: ", income_forest_test_error)
paste("This new model is more accurate than the tree model by:", income_tree_test_error - income_forest_test_error)
paste("This new model is more accurate than the regreession model by:", income_test_error - income_forest_test_error)
```
# Create a logistic regression model for the diabetes prediction
```{r}
fit <- glm(DIBEV_A_F ~ AGEP_A +
  SEX_A +
  MARITAL_A +
  REGION +
  HISPALLP_A +
  VISIONDF_A +
  BMICAT_A +
  HEARINGDF_A +
  DIFF_A
  +
  COMDIFF_A +
  COGMEMDFF_A, family = binomial, data = predict_diabetes.train)
summary(fit)
```
Marital Status, Hearing Difficulties and Communication Difficulties are insignificant and can be removed from the model

# Next we will fit the new training model as well as fit the model to the test dataset and create a confusion matrix.
## We will also be determining the rate of misclassification
```{r}
fit.1 <- glm(DIBEV_A_F ~ AGEP_A +
  SEX_A +
  REGION +
  HISPALLP_A +
  VISIONDF_A +
  BMICAT_A +
  DIFF_A +
  COGMEMDFF_A, family = binomial, data = predict_diabetes.train)
summary(fit.1)

fit.1.probs <- predict(fit.1, predict_diabetes.test, type = "response")
glm.pred <- rep(0, 10000)
glm.pred[fit.1.probs > .5] <- 1
glm.pred[fit.1.probs <= .5] <- 0
table(glm.pred, predict_diabetes.test$DIBEV_A_F)


diabetes_glm_model_rate_of_misclassification <- 1 - mean(glm.pred == predict_diabetes.test$DIBEV_A_F)
diabetes_glm_model_rate_of_misclassification
paste("The rate of miscalulation for the logistic regression model is:", diabetes_glm_model_rate_of_misclassification)
```

# Next we will fit the LDA to the Diabetes Model and calculate rate of misclassification
```{r}
lda.diabetes.fit.1 <- lda(DIBEV_A_F ~ AGEP_A +
  SEX_A +
  REGION +
  HISPALLP_A +
  VISIONDF_A +
  BMICAT_A +
  DIFF_A +
  COGMEMDFF_A, data = predict_diabetes.train)
diabetes.test <- predict_diabetes.test$DIBEV_A_F

lda.diabetes.pred <- predict(lda.diabetes.fit.1, predict_diabetes.test)

lda.diabetes.class <- lda.diabetes.pred$class

table(lda.diabetes.class, diabetes.test)

diabetes_lda_model_rate_of_misclassification <- 1 - mean(lda.diabetes.class == diabetes.test)

paste("The rate of miscalulation for the discriminant regression model for diabetes is:", diabetes_lda_model_rate_of_misclassification)
paste("This new model is less accurate than the logistic model by:", abs(diabetes_glm_model_rate_of_misclassification - diabetes_lda_model_rate_of_misclassification))
```

# Next we will run the diabetes prediction data through the classification tree model and determine the rate of misclassification
```{r}
tree.diabetes <- tree(as.factor(DIBEV_A_F) ~ AGEP_A +
  SEX_A +
  REGION +
  HISPALLP_A +
  VISIONDF_A +
  BMICAT_A +
  DIFF_A +
  COGMEMDFF_A, predict_diabetes.train)

tree.diabetes.pred <- predict(tree.diabetes, predict_diabetes.test, type = "class")

table(tree.diabetes.pred, as.factor(predict_diabetes.test$DIBEV_A_F))

diabetes_tree_model_rate_of_misclassification <- 1 - mean(tree.diabetes.pred == as.factor(predict_diabetes.test$DIBEV_A_F))

paste("The rate of miscalulation for the tree regression model is:", diabetes_tree_model_rate_of_misclassification)
paste("This new model is more accurate than the discriminant regression model by:", diabetes_lda_model_rate_of_misclassification - diabetes_tree_model_rate_of_misclassification)
paste("This new model is more accurate than the logistic model by:", diabetes_glm_model_rate_of_misclassification - diabetes_tree_model_rate_of_misclassification)
```

# I will now try to improve the diabetes tree model performance by creating a larger tree and pruning it
```{r}
set.seed(567)
cv.diabetes <- cv.tree(tree.diabetes, FUN = prune.misclass)
names(cv.diabetes)

par(mfrow = c(1, 2))
plot(cv.diabetes$size, cv.diabetes$dev, type = "b")
plot(cv.diabetes$k, cv.diabetes$dev, type = "b")

prune.diabetes <- prune.misclass(tree.diabetes, best = 4)
plot(prune.diabetes)
text(prune.diabetes, pretty = 0)

pruned.tree.diabetes.pred <- predict(prune.diabetes, predict_diabetes.test, type = "class")
table(pruned.tree.diabetes.pred, as.factor(predict_diabetes.test$DIBEV_A_F))

diabetes_pruned_tree_model_rate_of_misclassification <- 1 - mean(pruned.tree.diabetes.pred == as.factor(predict_diabetes.test$DIBEV_A_F))

paste("The rate of miscalulation for the tree regression model is:", diabetes_pruned_tree_model_rate_of_misclassification)
paste("This new model is more accurate than the discriminant regression model by:", diabetes_lda_model_rate_of_misclassification - diabetes_pruned_tree_model_rate_of_misclassification)
paste("This new model is more accurate than the logistic model by:", diabetes_glm_model_rate_of_misclassification - diabetes_pruned_tree_model_rate_of_misclassification)
paste("This new model is more accurate than the other tree model by:", diabetes_tree_model_rate_of_misclassification - diabetes_pruned_tree_model_rate_of_misclassification)
```

# Next I will create a random forest classification for the diabetes prediction and check the rate of misclassification
```{r}
set.seed(890)

rf.diabetes <- randomForest(DIBEV_A_F ~ AGEP_A +
  SEX_A +
  REGION +
  HISPALLP_A +
  VISIONDF_A +
  BMICAT_A +
  DIFF_A +
  COGMEMDFF_A, data = predict_diabetes.test, mtry = 3, importance = TRUE)
rf.diabetes

varImpPlot(rf.diabetes)
```

# I will now create some models to predict the amount of education each individual has.
## This first model will be using the LDA model
```{r}
lda.education.fit.1 <- lda(EDUC_A ~ AGEP_A +
  SEX_A +
  MARITAL_A +
  REGION +
  HISPALLP_A +
  VISIONDF_A +
  BMICAT_A +
  HEARINGDF_A +
  DIFF_A +
  COMDIFF_A +
  COGMEMDFF_A, data = predict_education.train)
education.test <- predict_education.test$EDUC_A

lda.education.pred <- predict(lda.education.fit.1, predict_education.test)

lda.education.class <- lda.education.pred$class

table(lda.education.class, education.test)

education_lda_model_rate_of_misclassification <- 1 - mean(lda.education.class == education.test)

paste("The rate of miscalulation for the discriminant regression model for education is:", education_lda_model_rate_of_misclassification)
```

# Next I will construct a tree model for the education prediction
```{r}
tree.education <- tree(as.factor(EDUC_A) ~ AGEP_A +
  SEX_A +
  REGION +
  HISPALLP_A +
  VISIONDF_A +
  BMICAT_A +
  DIFF_A +
  COGMEMDFF_A, predict_education.train)

tree.education.pred <- predict(tree.education, predict_education.test, type = "class")

table(tree.education.pred, as.factor(predict_education.test$EDUC_A))

education_tree_model_rate_of_misclassification <- 1 - mean(tree.education.pred == as.factor(predict_education.test$EDUC_A))

paste("The rate of miscalulation for the tree regression model is:", education_tree_model_rate_of_misclassification)
paste("This new model is less accurate than the discriminant regression model by:", abs(education_lda_model_rate_of_misclassification - education_tree_model_rate_of_misclassification))
```

# I will create a pruned tree in case it will improve model accuracy for the tree model
## Looks like the model is too small for pruning so this model will be skipped.
=== CODE IF NEEDED LATER ===
set.seed(135)

cv.education <- cv.tree(tree.education,FUN=prune.misclass)
names(cv.education)

par(mfrow = c(1,2))
plot(cv.education$size, cv.education$dev, type="b")
plot(cv.education$k, cv.education$dev, type="b")

prune.education <- prune.misclass(tree.education, best = 4)
plot(prune.education)
text(prune.education, pretty = 0)

pruned.tree.education.pred <- predict(prune.education, predict_education.test, type='class')
table(pruned.tree.education.pred, as.factor(predict_education.test$EDUC_A))

education_pruned_tree_model_rate_of_misclassification <- 1 - mean(pruned.tree.education.pred == as.factor(predict_education.test$v))

paste('The rate of misclassification for the tree regression model is:', education_pruned_tree_model_rate_of_misclassification)
paste('This new model is more accurate than the discriminant regression model by:', education_lda_model_rate_of_misclassification - diabetes_pruned_tree_model_rate_of_misclassification)
paste('This new model is more accurate than the other tree model by:',   education_tree_model_rate_of_misclassification - education_pruned_tree_model_rate_of_misclassification)
===END===

# Create a random forest model for education prediction and graph the model
```{r}
set.seed(79)

rf.education <- randomForest(EDUC_A ~ AGEP_A +
  SEX_A +
  MARITAL_A +
  REGION +
  HISPALLP_A +
  VISIONDF_A +
  BMICAT_A +
  HEARINGDF_A +
  DIFF_A +
  COMDIFF_A +
  COGMEMDFF_A, data = predict_education.test, mtry = 3, importance = TRUE)
rf.education

varImpPlot(rf.education)
```
